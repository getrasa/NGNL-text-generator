{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGNL Text Generator\n",
    "This is a LSTM RNN which will be trained on 5 volums of 'No Game No Life' light novel and will generate new text in that style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 200000\n",
      "Words: 29791\n",
      "Vocab: 8947\n"
     ]
    }
   ],
   "source": [
    "textdir = 'data/NGNL123.txt'\n",
    "with open(textdir, 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()[:200000]\n",
    "    \n",
    "words = text.split(\" \")\n",
    "vocab = list(set(words))\n",
    "print(\"Characters:\", len(text))\n",
    "print(\"Words:\", len(words))\n",
    "print(\"Vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print first 1000 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ufeff\\nPrologue\\nPart 1\\n--[Urban Legends].\\nThat was one of the [Desires] of which the amount almost numbered the\\nexpanse of stars in the sky.\\n--For example the Urban Legend that 'No Human has really ever stepped\\nfoot on the moon'.\\n--For example the conspiracy of the Freemasons in the American dollar bills.\\n--For example the time-space experiment that took place in Philadelphia.\\nThe Chiyoda Line’s nuclear shelter, Area 51, the Roswell UFO incident, etc.\\nAfter seeing this many examples, it can be concluded that there is always\\nsome kind of distinct pattern between them. That pattern is a...'It would be\\ninteresting if it was real' kind of [Desire].\\nSmoke will not rise without fire as its source. However as it spreads and\\nbuilds up, that [Desire] becomes a [Rumor].\\nThis kind of thinking was not strange to the world. Since ancient times,\\nHumans have always preferred the [Inevitable] to an [Accident], after all. The\\nbirth of mankind was likely the accidental product of astronomical\\nprobability. Ac\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Print first 1000 characters\")\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' QUOTATION_MARK ')\n",
    "    text = text.replace(\"'\", ' SINGLE_QUOTATION_MARK ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('[', ' <LEFT_BRACKET> ')\n",
    "    text = text.replace(']', ' <RIGHT_BRACKET> ')\n",
    "    text = text.replace('「', ' <LEFT_HALF_BRACKET> ')\n",
    "    text = text.replace('」', ' <RIGHT_HALF_BRACKET> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('-', ' <DASH> ')\n",
    "    text = text.replace('—', ' <STRAIGHT_LINE> ')\n",
    "    text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = [word for word in text.split(\" \") if word != '']\n",
    "    \n",
    "    \n",
    "    # Remove all words with 2 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] >= 2]\n",
    "    \n",
    "    return trimmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 51194 Vocab: 2218\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed words\n",
    "words = preprocessing(text)\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Words:\", len(words), \"Vocab:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2int = dict((c, i) for i, c in enumerate(vocab))\n",
    "int2vocab = dict((i, c) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(text, batch_size, len_per_section, skip):\n",
    "    batch_size = batch_size*skip\n",
    "    n_batches = (len(text) - len_per_section)// batch_size\n",
    "    \n",
    "    for idx in range(0, n_batches * batch_size, batch_size):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(idx, idx+batch_size, skip):\n",
    "            section = [vocab2int[word] for word in text[i:i+len_per_section]]\n",
    "            label = [vocab2int[text[i+len_per_section]]]\n",
    "            \n",
    "            # One hot encode input\n",
    "            one_hot_input = np.zeros((len_per_section, vocab_size))\n",
    "            one_hot_label = np.zeros(vocab_size)\n",
    "            \n",
    "            for j, word in enumerate(section):\n",
    "                one_hot_input[j][word] = 1\n",
    "                \n",
    "            one_hot_label[label] = 1\n",
    "                \n",
    "            x.append(one_hot_input)\n",
    "            y.append(one_hot_label)\n",
    "\n",
    "        yield np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIT\n",
      "[0] [['<NEW_LINE>', '<NEW_LINE>', 'part', '1', '<NEW_LINE>', '<HYPHENS>']]\n",
      "SHIT\n",
      "['<LEFT_BRACKET>'] [['<NEW_LINE>', 'part', '1', '<NEW_LINE>', '<HYPHENS>', '<LEFT_BRACKET>']]\n",
      "SHIT\n",
      "['urban'] [['part', '1', '<NEW_LINE>', '<HYPHENS>', '<LEFT_BRACKET>', 'urban']]\n",
      "SHIT\n",
      "['legends'] [['1', '<NEW_LINE>', '<HYPHENS>', '<LEFT_BRACKET>', 'urban', 'legends']]\n",
      "SHIT\n",
      "['<RIGHT_BRACKET>'] [['<NEW_LINE>', '<HYPHENS>', '<LEFT_BRACKET>', 'urban', 'legends', '<RIGHT_BRACKET>']]\n",
      "SHIT\n",
      "['<PERIOD>'] [['<HYPHENS>', '<LEFT_BRACKET>', 'urban', 'legends', '<RIGHT_BRACKET>', '<PERIOD>']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['<LEFT_BRACKET>', 'urban', 'legends', '<RIGHT_BRACKET>', '<PERIOD>', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['that'] [['urban', 'legends', '<RIGHT_BRACKET>', '<PERIOD>', '<NEW_LINE>', 'that']]\n",
      "SHIT\n",
      "['was'] [['legends', '<RIGHT_BRACKET>', '<PERIOD>', '<NEW_LINE>', 'that', 'was']]\n",
      "SHIT\n",
      "['one'] [['<RIGHT_BRACKET>', '<PERIOD>', '<NEW_LINE>', 'that', 'was', 'one']]\n",
      "SHIT\n",
      "['of'] [['<PERIOD>', '<NEW_LINE>', 'that', 'was', 'one', 'of']]\n",
      "SHIT\n",
      "['the'] [['<NEW_LINE>', 'that', 'was', 'one', 'of', 'the']]\n",
      "SHIT\n",
      "['<LEFT_BRACKET>'] [['that', 'was', 'one', 'of', 'the', '<LEFT_BRACKET>']]\n",
      "SHIT\n",
      "['<RIGHT_BRACKET>'] [['was', 'one', 'of', 'the', '<LEFT_BRACKET>', '<RIGHT_BRACKET>']]\n",
      "SHIT\n",
      "['of'] [['one', 'of', 'the', '<LEFT_BRACKET>', '<RIGHT_BRACKET>', 'of']]\n",
      "SHIT\n",
      "['which'] [['of', 'the', '<LEFT_BRACKET>', '<RIGHT_BRACKET>', 'of', 'which']]\n",
      "SHIT\n",
      "['the'] [['the', '<LEFT_BRACKET>', '<RIGHT_BRACKET>', 'of', 'which', 'the']]\n",
      "SHIT\n",
      "['amount'] [['<LEFT_BRACKET>', '<RIGHT_BRACKET>', 'of', 'which', 'the', 'amount']]\n",
      "SHIT\n",
      "['almost'] [['<RIGHT_BRACKET>', 'of', 'which', 'the', 'amount', 'almost']]\n",
      "SHIT\n",
      "['the'] [['of', 'which', 'the', 'amount', 'almost', 'the']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['which', 'the', 'amount', 'almost', 'the', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['expanse'] [['the', 'amount', 'almost', 'the', '<NEW_LINE>', 'expanse']]\n",
      "SHIT\n",
      "['of'] [['amount', 'almost', 'the', '<NEW_LINE>', 'expanse', 'of']]\n",
      "SHIT\n",
      "['stars'] [['almost', 'the', '<NEW_LINE>', 'expanse', 'of', 'stars']]\n",
      "SHIT\n",
      "['in'] [['the', '<NEW_LINE>', 'expanse', 'of', 'stars', 'in']]\n",
      "SHIT\n",
      "['the'] [['<NEW_LINE>', 'expanse', 'of', 'stars', 'in', 'the']]\n",
      "SHIT\n",
      "['sky'] [['expanse', 'of', 'stars', 'in', 'the', 'sky']]\n",
      "SHIT\n",
      "['<PERIOD>'] [['of', 'stars', 'in', 'the', 'sky', '<PERIOD>']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['stars', 'in', 'the', 'sky', '<PERIOD>', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['<HYPHENS>'] [['in', 'the', 'sky', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>']]\n",
      "SHIT\n",
      "['for'] [['the', 'sky', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for']]\n",
      "SHIT\n",
      "['example'] [['sky', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for', 'example']]\n",
      "SHIT\n",
      "['the'] [['<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for', 'example', 'the']]\n",
      "SHIT\n",
      "['urban'] [['<NEW_LINE>', '<HYPHENS>', 'for', 'example', 'the', 'urban']]\n",
      "SHIT\n",
      "['legend'] [['<HYPHENS>', 'for', 'example', 'the', 'urban', 'legend']]\n",
      "SHIT\n",
      "['that'] [['for', 'example', 'the', 'urban', 'legend', 'that']]\n",
      "SHIT\n",
      "['SINGLE_QUOTATION_MARK'] [['example', 'the', 'urban', 'legend', 'that', 'SINGLE_QUOTATION_MARK']]\n",
      "SHIT\n",
      "['no'] [['the', 'urban', 'legend', 'that', 'SINGLE_QUOTATION_MARK', 'no']]\n",
      "SHIT\n",
      "['human'] [['urban', 'legend', 'that', 'SINGLE_QUOTATION_MARK', 'no', 'human']]\n",
      "SHIT\n",
      "['has'] [['legend', 'that', 'SINGLE_QUOTATION_MARK', 'no', 'human', 'has']]\n",
      "SHIT\n",
      "['really'] [['that', 'SINGLE_QUOTATION_MARK', 'no', 'human', 'has', 'really']]\n",
      "SHIT\n",
      "['ever'] [['SINGLE_QUOTATION_MARK', 'no', 'human', 'has', 'really', 'ever']]\n",
      "SHIT\n",
      "['stepped'] [['no', 'human', 'has', 'really', 'ever', 'stepped']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['human', 'has', 'really', 'ever', 'stepped', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['on'] [['has', 'really', 'ever', 'stepped', '<NEW_LINE>', 'on']]\n",
      "SHIT\n",
      "['the'] [['really', 'ever', 'stepped', '<NEW_LINE>', 'on', 'the']]\n",
      "SHIT\n",
      "['SINGLE_QUOTATION_MARK'] [['ever', 'stepped', '<NEW_LINE>', 'on', 'the', 'SINGLE_QUOTATION_MARK']]\n",
      "SHIT\n",
      "['<PERIOD>'] [['stepped', '<NEW_LINE>', 'on', 'the', 'SINGLE_QUOTATION_MARK', '<PERIOD>']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['<NEW_LINE>', 'on', 'the', 'SINGLE_QUOTATION_MARK', '<PERIOD>', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['<HYPHENS>'] [['on', 'the', 'SINGLE_QUOTATION_MARK', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>']]\n",
      "SHIT\n",
      "['for'] [['the', 'SINGLE_QUOTATION_MARK', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for']]\n",
      "SHIT\n",
      "['example'] [['SINGLE_QUOTATION_MARK', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for', 'example']]\n",
      "SHIT\n",
      "['the'] [['<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for', 'example', 'the']]\n",
      "SHIT\n",
      "['of'] [['<NEW_LINE>', '<HYPHENS>', 'for', 'example', 'the', 'of']]\n",
      "SHIT\n",
      "['the'] [['<HYPHENS>', 'for', 'example', 'the', 'of', 'the']]\n",
      "SHIT\n",
      "['in'] [['for', 'example', 'the', 'of', 'the', 'in']]\n",
      "SHIT\n",
      "['the'] [['example', 'the', 'of', 'the', 'in', 'the']]\n",
      "SHIT\n",
      "['<PERIOD>'] [['the', 'of', 'the', 'in', 'the', '<PERIOD>']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['of', 'the', 'in', 'the', '<PERIOD>', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['<HYPHENS>'] [['the', 'in', 'the', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>']]\n",
      "SHIT\n",
      "['for'] [['in', 'the', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for']]\n",
      "SHIT\n",
      "['example'] [['the', '<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for', 'example']]\n",
      "SHIT\n",
      "['the'] [['<PERIOD>', '<NEW_LINE>', '<HYPHENS>', 'for', 'example', 'the']]\n",
      "SHIT\n",
      "['time'] [['<NEW_LINE>', '<HYPHENS>', 'for', 'example', 'the', 'time']]\n",
      "SHIT\n",
      "['<DASH>'] [['<HYPHENS>', 'for', 'example', 'the', 'time', '<DASH>']]\n",
      "SHIT\n",
      "['space'] [['for', 'example', 'the', 'time', '<DASH>', 'space']]\n",
      "SHIT\n",
      "['experiment'] [['example', 'the', 'time', '<DASH>', 'space', 'experiment']]\n",
      "SHIT\n",
      "['that'] [['the', 'time', '<DASH>', 'space', 'experiment', 'that']]\n",
      "SHIT\n",
      "['took'] [['time', '<DASH>', 'space', 'experiment', 'that', 'took']]\n",
      "SHIT\n",
      "['place'] [['<DASH>', 'space', 'experiment', 'that', 'took', 'place']]\n",
      "SHIT\n",
      "['in'] [['space', 'experiment', 'that', 'took', 'place', 'in']]\n",
      "SHIT\n",
      "['<PERIOD>'] [['experiment', 'that', 'took', 'place', 'in', '<PERIOD>']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['that', 'took', 'place', 'in', '<PERIOD>', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['the'] [['took', 'place', 'in', '<PERIOD>', '<NEW_LINE>', 'the']]\n",
      "SHIT\n",
      "['shelter'] [['place', 'in', '<PERIOD>', '<NEW_LINE>', 'the', 'shelter']]\n",
      "SHIT\n",
      "['<COMMA>'] [['in', '<PERIOD>', '<NEW_LINE>', 'the', 'shelter', '<COMMA>']]\n",
      "SHIT\n",
      "['area'] [['<PERIOD>', '<NEW_LINE>', 'the', 'shelter', '<COMMA>', 'area']]\n",
      "SHIT\n",
      "['<COMMA>'] [['<NEW_LINE>', 'the', 'shelter', '<COMMA>', 'area', '<COMMA>']]\n",
      "SHIT\n",
      "['the'] [['the', 'shelter', '<COMMA>', 'area', '<COMMA>', 'the']]\n",
      "SHIT\n",
      "['<COMMA>'] [['shelter', '<COMMA>', 'area', '<COMMA>', 'the', '<COMMA>']]\n",
      "SHIT\n",
      "['etc'] [['<COMMA>', 'area', '<COMMA>', 'the', '<COMMA>', 'etc']]\n",
      "SHIT\n",
      "['<PERIOD>'] [['area', '<COMMA>', 'the', '<COMMA>', 'etc', '<PERIOD>']]\n",
      "SHIT\n",
      "['<NEW_LINE>'] [['<COMMA>', 'the', '<COMMA>', 'etc', '<PERIOD>', '<NEW_LINE>']]\n",
      "SHIT\n",
      "['after'] [['the', '<COMMA>', 'etc', '<PERIOD>', '<NEW_LINE>', 'after']]\n",
      "SHIT\n",
      "['seeing'] [['<COMMA>', 'etc', '<PERIOD>', '<NEW_LINE>', 'after', 'seeing']]\n",
      "SHIT\n",
      "['this'] [['etc', '<PERIOD>', '<NEW_LINE>', 'after', 'seeing', 'this']]\n",
      "SHIT\n",
      "['many'] [['<PERIOD>', '<NEW_LINE>', 'after', 'seeing', 'this', 'many']]\n",
      "SHIT\n",
      "['examples'] [['<NEW_LINE>', 'after', 'seeing', 'this', 'many', 'examples']]\n",
      "SHIT\n",
      "['<COMMA>'] [['after', 'seeing', 'this', 'many', 'examples', '<COMMA>']]\n",
      "SHIT\n",
      "['it'] [['seeing', 'this', 'many', 'examples', '<COMMA>', 'it']]\n",
      "SHIT\n",
      "['can'] [['this', 'many', 'examples', '<COMMA>', 'it', 'can']]\n",
      "SHIT\n",
      "['be'] [['many', 'examples', '<COMMA>', 'it', 'can', 'be']]\n",
      "SHIT\n",
      "['concluded'] [['examples', '<COMMA>', 'it', 'can', 'be', 'concluded']]\n",
      "SHIT\n",
      "['that'] [['<COMMA>', 'it', 'can', 'be', 'concluded', 'that']]\n"
     ]
    }
   ],
   "source": [
    "# Test generator (not important)\n",
    "\n",
    "gen = get_batches(words[:100], 1, 6, 1)\n",
    "prev_label = [0]\n",
    "for x,y in gen:\n",
    "    shit = []\n",
    "    \n",
    "    for j, i in enumerate(x):\n",
    "        shit.append([int2vocab[np.argmax(num)] for num in i])\n",
    "        \n",
    "    out = [num for num in shit]\n",
    "    if prev_label[0] != out[:][-1]:\n",
    "        print(\"SHIT\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Works\")\n",
    "    \n",
    "    print(prev_label, out)\n",
    "    prev_label = [int2vocab[np.argmax(label)] for label in y]\n",
    "    \n",
    "    \n",
    "        \n",
    "    #print(\"INPUT\",[num for num in shit], \"\\n\", \"Label\", [int2vocab[np.argmax(label)] for label in y], \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "hidden_nodes = 512\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "len_per_section = 15\n",
    "skip = 1\n",
    "start_quote = \"I thought that we could\"\n",
    "\n",
    "log_every = 5\n",
    "test_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, [batch_size, len_per_section, vocab_size], name='input')\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, vocab_size], name='labels')\n",
    "    \n",
    "    ## Initialize weights \n",
    "    # Forget Gate weights\n",
    "    w_fi = tf.Variable(tf.truncated_normal([vocab_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Input Gate weights\n",
    "    w_ii = tf.Variable(tf.truncated_normal([vocab_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Memory cell weights\n",
    "    w_ci = tf.Variable(tf.truncated_normal([vocab_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Output Gate weights\n",
    "    w_oi = tf.Variable(tf.truncated_normal([vocab_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # LSTM Cell\n",
    "    def lstm(i, o, state):\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        memory_gate = tf.tanh(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        \n",
    "        state = forget_gate * state + input_gate * memory_gate\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    # LSTM\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "    \n",
    "    \n",
    "    for i in range(len_per_section):\n",
    "        output, state = lstm(inputs[:, i, :], output, state)\n",
    "        \n",
    "        if (i==0):\n",
    "            output_all_i = output\n",
    "            labels_all_i = inputs[:, i+1, :]\n",
    "            \n",
    "        elif i == len_per_section - 1:\n",
    "            output_all_i = tf.concat([output_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, labels], 0)\n",
    "            \n",
    "        else:\n",
    "            output_all_i = tf.concat([output_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, inputs[:, i+1,:]], 0)\n",
    "            \n",
    "\n",
    "    #Classifier\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, vocab_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocab_size]))\n",
    "    logits = tf.matmul(output_all_i, w) + b\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels_all_i))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    #Test\n",
    "    test_input = tf.placeholder(tf.float32, [1, vocab_size])\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])),\n",
    "                               test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "    \n",
    "    test_output, test_state = lstm(test_input, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch    0/199   train_loss = 7.802   time: 2017-12-30 07:06:16.193469\n",
      "================================================================================\n",
      "I thought that we could itself statement pockets surprised eastern light filled read wars deus itself statement pockets surprised eastern light filled read wars deus itself statement pockets surprised eastern light filled read wars deus itself statement pockets surprised eastern light filled read wars deus itself statement pockets surprised eastern light filled read wars deus itself statement pockets surprised eastern light filled read wars deus itself statement pockets surprised eastern light filled read wars deus itself statement pockets surprised eastern light filled read wars deus\n",
      "================================================================================\n",
      "Epoch   1 Batch    5/199   train_loss = 7.743   time: 2017-12-30 07:06:42.889344\n",
      "Epoch   1 Batch   10/199   train_loss = 7.616   time: 2017-12-30 07:07:11.511975\n",
      "================================================================================\n",
      "I thought that we could itself statement pockets surprised eastern light filled read charisma without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without without\n",
      "================================================================================\n",
      "Epoch   1 Batch   15/199   train_loss = 7.559   time: 2017-12-30 07:07:42.776844\n",
      "Epoch   1 Batch   20/199   train_loss = 7.548   time: 2017-12-30 07:08:27.248702\n",
      "================================================================================\n",
      "I thought that we could itself statement knelt panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked\n",
      "================================================================================\n",
      "Epoch   1 Batch   25/199   train_loss = 7.443   time: 2017-12-30 07:08:55.774215\n",
      "Epoch   1 Batch   30/199   train_loss = 7.376   time: 2017-12-30 07:09:24.185075\n",
      "================================================================================\n",
      "I thought that we could itself statement knelt panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked knowing panicked\n",
      "================================================================================\n",
      "Epoch   1 Batch   35/199   train_loss = 7.346   time: 2017-12-30 07:09:52.958794\n",
      "Epoch   1 Batch   40/199   train_loss = 7.148   time: 2017-12-30 07:10:22.137889\n",
      "================================================================================\n",
      "I thought that we could itself statement <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters <PERIOD> nighters\n",
      "================================================================================\n",
      "Epoch   1 Batch   45/199   train_loss = 7.005   time: 2017-12-30 07:10:51.189272\n",
      "Epoch   1 Batch   50/199   train_loss = 7.169   time: 2017-12-30 07:11:20.566465\n",
      "================================================================================\n",
      "I thought that we could <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD> <PERIOD>\n",
      "================================================================================\n",
      "Epoch   1 Batch   55/199   train_loss = 6.964   time: 2017-12-30 07:11:50.173242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-7a0730231d2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_per_section\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matko\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matko\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matko\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matko\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matko\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    len_batches = len(list(get_batches(words, batch_size, len_per_section, skip)))\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        batches = get_batches(words, batch_size, len_per_section, skip)\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            train_loss, _ = sess.run([loss, optimizer], feed_dict={inputs: x, labels: y})\n",
    "            \n",
    "            if (batch_i*epoch_i) % log_every == 0:\n",
    "                print(\"Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}   time: {}\".format(epoch_i, \n",
    "                                                                                           batch_i, \n",
    "                                                                                           len_batches, \n",
    "                                                                                           train_loss,\n",
    "                                                                                          datetime.datetime.now()))\n",
    "                \n",
    "                \n",
    "            if (batch_i * epoch_i % test_every == 0):\n",
    "                reset_test_state.run()\n",
    "                generated_text = start_quote\n",
    "                quote_words = start_quote.lower().split(\" \")\n",
    "                \n",
    "                \n",
    "                for i in range(len(quote_words) - 1):\n",
    "                    testX = np.zeros((1, vocab_size))\n",
    "                    testX[0, vocab2int[quote_words[i]]] = 1\n",
    "                    _ = sess.run(test_prediction, feed_dict={test_input: testX})\n",
    "                    \n",
    "                testX = np.zeros((1, vocab_size))\n",
    "                testX[0, vocab2int[quote_words[-1]]] = 1\n",
    "                \n",
    "                for i in range(80):\n",
    "                    prediction = test_prediction.eval({test_input: testX})\n",
    "                    next_word = int2vocab[np.argmax(prediction)]\n",
    "                    generated_text += \" \" + next_word\n",
    "                    testX = np.zeros((1, vocab_size))\n",
    "                    testX[0, np.argmax(prediction)] = 1\n",
    "                    \n",
    "                print('=' * 80)\n",
    "                print(generated_text)\n",
    "                print('=' * 80)\n",
    "                \n",
    "                \n",
    "    # Save Model\n",
    "    #saver = tf.train.Saver()\n",
    "    #saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
